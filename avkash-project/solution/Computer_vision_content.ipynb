{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Analysis\n",
    "- https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/intro-to-spatial-analysis-public-preview\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating flight manifest\n",
    "\n",
    "- Flight No | Origin | Destination  | Date | Time | First Name | Last Name | Seat # | Date of Birth\n",
    "\n",
    "\n",
    "## Data Provided by the passanger \n",
    "- Boarding Pass\n",
    "    - First Name, Last Name\n",
    "    - Seat # \n",
    "    - Date \n",
    "    - Flight No\n",
    "    - Origin and Destination\n",
    "- Driving License ID (Optional Passport Image)\n",
    "  - First Name, Last Name, DoB\n",
    "  - Face Pic\n",
    "- The Passanger Photo/30 second video\n",
    "  - Considered the real face seen by the Airport Kiosk\n",
    "  - This 30 second video is considered the real-time view of passener at the Kiosk \n",
    "\n",
    "## How it works?\n",
    "- Input \n",
    "    - The passenger provides boarding pass and ID \n",
    "    - Passenger also provides a 30 second video to \n",
    "    - Passenger baggage xray photos are provided\n",
    "- Output\n",
    "    - Kisok Identifies and display the following message:\n",
    "        - Dear {Mr|Mrs} X Y, \n",
    "        - You are welcome to the today's flight No XXXX from Origin to Destination\n",
    "        - Your seat # is XYZ\n",
    "        - The weather at your destination is XXX and the temperature is NNN\n",
    "    - Kiosk also does the following internal processing\n",
    "        - ID photo validation matched with given photo - X% above threshold\n",
    "        - Match the other passengers from the same family also \n",
    "        - Collect passenger emotion as positive or negative feedback \n",
    "    - Kiosk also checks the baggage photos to identify prohibited items\n",
    "        - flag baggage for prohibited items and recognize the correct owner from ID and Boarding pass\n",
    "    - Upload all the validated data to server as table\n",
    "\n",
    "## Exercise:\n",
    "- Create form recognizer for boarding pass identification to extract info\n",
    "    - Get \n",
    "        First Name, Last Name | Seat # | Flight Date | Flight No | Origin | Destination\n",
    "- Use pre-train ID recognizer to extract ID specific info\n",
    "    - Get Male | Female\n",
    "    - Get First Name, Last Name, Dob\n",
    "- Use passenger photos to identify person in the photo\n",
    "- Use 30 second video to extract\n",
    "    - Person photos and recognize the identiy\n",
    "    - Identify emotion in the photo and display at the prompt\n",
    "    - Collect emotion as overall Kios feedback \n",
    "- Identify prohibited items in the luggage from custom model\n",
    "    - Build a custom model to identify {Gun/knife/hand grenade} from x-ray luggage images\n",
    "- Read data from Azure Blob storage\n",
    "- Write data back to Azure Blob Storage\n",
    "\n",
    "\n",
    "## Standout Topics\n",
    "- Check current time and see if the person is 4 hours within the flight time or Late at the gate\n",
    "- Passenger can drop their business card and a winner will be decided to will a free flight ticket\n",
    "    - Create a system to collect business card data using Business Card recognizer\n",
    "    - Save all the colleted info into a table\n",
    "    - Select a random winner from the business user list\n",
    "- Use a 30 second video of a celebrity and match him/her with flight manifest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline and Duration - Total 5 Hours\n",
    "\n",
    "- ### A. Problem Definition & System Design (objective 1) - (60 min)\n",
    "    - Problem Details\n",
    "    - Flight Manifest, Saved at Azure Blob\n",
    "    - Boarding Pass desing and passenger boarding pass\n",
    "    - End to End project walkthrough\n",
    "    - Describe Input Data\n",
    "    - Describe Output Data\n",
    "    - Kiosk Design and Greeting's Display\n",
    "\n",
    "- ### B. Applying Azure Form Recognizer to collect text date from various images (60 Min)\n",
    "    - Collect personal information from driving license using pre-built ID recognizer\n",
    "    - Collect personal information from Business Card using pre-built business card recognizer\n",
    "    - Create a custom form recognier model to collect text data from a flight boarding pass\n",
    "    - Use custom boarding pass recognizer model to collect passenger information from a flight boarding pass \n",
    "    - Cross reference the boarding pass information from flight manifest \n",
    "\n",
    "\n",
    "- ### C. Applying Azure Facial Recognition, Video Analysis & Insight to index and analyze faces and emotions within faces (60 min)\n",
    "    - Based on a collection of individual photos, build a model to identify a person \n",
    "    - Extract different emotion from faces using pre-built model\n",
    "    - Process video to generate key-frames\n",
    "    - Collect faces from the video and then Identify faces from the collect frames\n",
    "    - Collect emotions from faces in the video\n",
    "\n",
    "\n",
    "- ### D. Applying Azure Custom Vision to create custom image recognition model (60 Min)\n",
    "    - Creating a custom Image Recognition Model using prohibited item inside checked baggage\n",
    "    - Train and validate the custom model\n",
    "    - Use the model to identify and flag suspected luggage from xray vision images\n",
    "\n",
    "- ### E. Combining everything together (30 Min)\n",
    "    - Create the final solution by integrating various data streams and general final results\n",
    "    - Display Kiosk message based on various data inputs and analysis\n",
    "    - Upload final result data and analytics securely at Azure blob as update flight manifest \n",
    "\n",
    "- ### F. Application Deployment and Monitoring (30 Min)\n",
    "    - Deploy and monitor system performance from Azure Portal. \n",
    "    - Manage/Monitor system via console and create performance report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objective 1: Design computer vision applications to solve a business problem based on a given use case\n",
    "- Describe considerations for creating computer vision applications\n",
    "- Identify Azure computer vision services for solutions given a scenario\n",
    "- Identify data considerations and sources for Azure computer vision solutions\n",
    "- Provision and consume Azure computer vision APIs\n",
    "- Describe disclosure recommendations related to spatial or facial recognition\n",
    "- Exercises: Given a business scenario or use case, create requirements documentation, architecture diagrams, and high level descriptions of solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objective 2: Extract text information from digital content within a business use case or scenario\n",
    "- Extracting personal, business and documnt specific information from various digital documents i.e. driving license, business card, receipts etc using pre-built recognizer\n",
    "- Create a custom form recognier model to collect text data from a digital document\n",
    "- Collecting processed information, analyze based on provided logic and the store into azure blob storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objective 3: Process video and images to extract faces and facial landmarks, emotions and various other face specific information\n",
    "- Based on a collection of individual photos, build a model to identify a person \n",
    "- Processing video to generate key-frames, collecting and grouping faces, match with existing known (train) faces\n",
    "- Collect emotions from faces using pre-built model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Objective 4: Creating custom image classification and object detection solutions\n",
    "- Using image classification and object detection pre-build services and pre-trained model in Azure Computer vision service to identify various object types\n",
    "- Training a custom image classification and object detection model by providing various images and then using traing model in different business specific usage\n",
    "- Improve custom model performance and then deploying as prediction service endpoint, using prediction service programmaticaly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objective 5: Building, deploying and finally monitoring Azure computer vision solutions  \n",
    "\n",
    "- Azure computer vision solution Deplyment, monitoring and Management, \n",
    "- Adding security to application along with extended logging and enhance diagnostics \n",
    "- Cost estimation strategy, Ethical and Bias consideration of AI solutions and data collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing Project Details\n",
    "\n",
    "## Step 1: Problem Definition and System Design\n",
    "\n",
    "The purpose of this stage is to set the table for the whole project.  You must make the connection between the technical tools and the value-add for all stakeholders, as well as consider the ethical implications.  Write enough to get your point across for one of the functional areas: luggage X-rays, ID card verification, or emotion extraction.  After that is complete, draw an architecture diagram outlining how you expect the tools to interact in this proof of concept.\n",
    "\n",
    "\n",
    "## Helpful notes:\n",
    "A submitted text document should have at least three sections.  \n",
    "\n",
    "### Example 1:\n",
    "- luggage x-ray system:\n",
    "- Airport & Airline impact\n",
    "- Customer Impact\n",
    "- Ethical & Privacy considerations\n",
    "- Example 1:\n",
    "\n",
    "### ID card verification system:\n",
    "- Airport & Airline impact\n",
    "- Customer Impact\n",
    "- Ethical & Privacy considerations\n",
    "\n",
    "### Emotion Extraction\n",
    "- Airport & Airline impact\n",
    "- Customer Impact\n",
    "- Ethical & Privacy considerations\n",
    "\n",
    "\n",
    "\n",
    "### Submission Materials:  \n",
    "- Business Case & Ethics Document (docx, doc, or pdf) \n",
    "- Architecture Diagram (pdf, jpg)\n",
    "\n",
    "\n",
    "### Step 2: Luggage Computer Vision model\n",
    "The purpose of this step is to create the custom computer vision model to label x-ray images as containing a \"threat\" or \"no threat\".  We will be implementing the cognitive service \"custom cv\" as defined in the architecture document.\n",
    "Utilize the customvision.ai toolkit to upload the relevant dataset.\n",
    "\n",
    "- Annotate the classes and train a custom model.\n",
    "- Expose the API endpoint. \n",
    "\n",
    "Take a screenshot of the blocks of code and the outcome of the code block.  (jpg, png)\n",
    "\n",
    "### Step 3:  Facial Detection and Emotion Extraction\n",
    "The purpose of this step is to utilize Azure's Face Cognitive Service, and specifically the emotion extraction method enabled by Azure's transfer learning. Since the model is pre-trained the primary concern is the engineering work connecting API keys and ensuring the test data hits the API correctly.\n",
    "- Use the ML Studio notebook environment to configure the Face Cognitive Service API.\n",
    "- Verify the output with the provided test images.\n",
    "\n",
    "Take a screenshot of the blocks of code and the outcome of the code block.  (jpg, png)\n",
    "\n",
    "\n",
    "### Step 4: OCR and Text Extraction\n",
    "This step follows a similar pattern to the Facial Detection work. The cognitive service is pre-trained thanks to the Microsoft team of scientists, and we get to take advantage of transfer learning to extract information from identification cards.  \n",
    "- Build a notebook in ML studio which configures the Form Recognizer API.\n",
    "- Verify the output with the provided test images.\n",
    "\n",
    "Take a screenshot of the blocks of code and the outcome of the code block.  (jpg, png)\n",
    "\n",
    "### Step 5: Write test results to Blob Storage\n",
    "Now that the models are all functioning and have successfully called the test images, we need to write the results to blob storage. This makes the data available to other services and preserves the results outside of the Machine Learning Studio Environment.\n",
    "- For all the model notebooks, add code that writes the API call results to blob storage in an appropriate format.\n",
    "\n",
    "Provide a download of the written files in the final submission (txt, parquet, csv)\n",
    "\n",
    "### Step 6: Monitor the Solution\n",
    "The last step is to enable the metric reporting for all of the cognitive services created from the logging and monitoring dashboard. This procedure will allow the user to keep an eye on uptime and total usage of any application.\n",
    "- Build total call metrics dashboard for each solution\n",
    "- Verify the real-time cost of the system and total data consumption\n",
    "\n",
    "Take a screenshot of the created dashboard for each solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
